{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model Development (Individual)\n",
    "\n",
    "      Cross_Sell_Success_Dataset_2023\n",
    "      \n",
    " Name: Piyush Kumar\n",
    " \n",
    " Due Date: March 3\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The development of a classification model is the process of building a machine learning model that can accurately divide data into predefined classes or categories. Image recognition, sentiment analysis, fraud detection, and spam filtering are all common uses for this kind of model. In this context, a dataset containing customer data for a meal delivery service has been provided. The development of a classification model typically involves the following key steps:\n",
    "\n",
    "Data collection and preparation: A dataset with labelled examples of the classes you want the model to classify must be gathered and prepared for this. To ensure that it accurately depicts the real-world scenarios that the model will use to classify, the dataset needs to be carefully curated. \n",
    "\n",
    "Feature selection and engineering:  Changing the raw data into a more meaningful representation or choosing a subset of features that are most crucial to the task at hand may be necessary.\n",
    "\n",
    "Model selection and training: To determine whether a customer will purchase the new product line, classification models such as logistic regression, decision tree classifier, random forest classifier, gradient boosting classifier, and kneighbors will be developed. \n",
    "\n",
    "Model evaluation and tuning: The next step is to evaluate the model's performance and adjust its parameters to increase accuracy after it has been trained. \n",
    "\n",
    "Deployment: The company will be able to conduct more targeted marketing campaigns and increase its cross-selling success rate as a result of this analysis, which aims to develop a model that accurately predicts the likelihood of a customer purchasing the new product line.\n",
    "For the purpose of predicting a dataset's cross-sell success, we developed a number of classification models for this project. Cross_sell_success becomes the target variable in this instance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4s4RNcYPYm-A"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel('Cross_Sell_Success_Dataset_2023.xlsx')\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Set random state\n",
    "random_state = 219\n",
    "\n",
    "# function to plot confusion matrix as heatmap\n",
    "def plot_heatmap(confusion_matrix):\n",
    "  # Define the labels for the classes\n",
    "  class_names = ['0', '1']\n",
    "\n",
    "  # Plot the heatmap\n",
    "  sns.heatmap(confusion_matrix, annot=True, cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "\n",
    "  # Add labels and title\n",
    "  plt.xlabel('Predicted labels')\n",
    "  plt.ylabel('True labels')\n",
    "  plt.title('Confusion Matrix')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading the data from the excel dataset. \n",
    "cross_df = pd.read_excel('Cross_Sell_Success_Dataset_2023.xlsx')\n",
    "\n",
    "cross_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking the characteristics of the dataframe\n",
    "cross_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVcGDbA4Ym-G"
   },
   "source": [
    "Checking correlation to find which parameters are best for predicting the REVENUE value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cUR6gB2Ym-I",
    "outputId": "ad6be8d9-b8d7-4c89-eef6-50566f923410"
   },
   "outputs": [],
   "source": [
    "# Check correlation\n",
    "\n",
    "print(df.columns)\n",
    "correlation = df.corr()['CROSS_SELL_SUCCESS'].sort_values(ascending=False)\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pL9sNbAsYm-K"
   },
   "outputs": [],
   "source": [
    "# dropping unneccessary columns\n",
    "X = df.drop(['CROSS_SELL_SUCCESS', 'EMAIL'], axis=1)\n",
    "\n",
    "# output prediction column\n",
    "y = df['CROSS_SELL_SUCCESS']\n",
    "\n",
    "# splitting dataset to training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=219, stratify=y)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbOJOQWqYm-L"
   },
   "source": [
    "## 1. Developing Logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression, a popular classification method, is frequently used in binary classification tasks. We will go over the steps involved in creating a logistic regression classifier in this section. When the relationship between the input features and the target variable is roughly linear, a logistic regression classifier can be very effective for classification tasks. However, it might struggle with relationships between features and the target variable that are more complicated and non-linear.\n",
    "\n",
    "\n",
    "Steps:\n",
    "For data manipulation, import the necessary libraries, such as pandas, scikit-learn, and matplotlib for visualizations.\n",
    "Separate the target variable from the predictor variables before loading the data into a Pandas dataframe. Then calculate the accuracy, train test gap and auc score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQe8T-WYYm-M"
   },
   "outputs": [],
   "source": [
    "model_name = \"Logistic Regression\"\n",
    "# Create and Fit Logistic regression model\n",
    "lr = LogisticRegression(random_state=random_state)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# predictions on train and test\n",
    "lr_train_preds = lr.predict(X_train)\n",
    "lr_test_preds = lr.predict(X_test)\n",
    "\n",
    "# accuracy calculation\n",
    "lr_train_accuracy = accuracy_score(y_train, lr_train_preds).round(4)\n",
    "lr_test_accuracy = accuracy_score(y_test, lr_test_preds).round(4)\n",
    "\n",
    "# train test gap\n",
    "lr_train_test_gap = abs(lr_train_accuracy - lr_test_accuracy).round(4)\n",
    "\n",
    "# auc score\n",
    "lr_auc = roc_auc_score(y_test, lr.predict_proba(X_test)[:,1]).round(4)\n",
    "\n",
    "# confusion matrix\n",
    "lr_cm = confusion_matrix(y_test, lr_test_preds)\n",
    "\n",
    "model_summary =  f\"\"\"\\\n",
    "Model Name:     {model_name}\n",
    "Train Accuracy: {lr_train_accuracy}\n",
    "Test Accuracy:  {lr_test_accuracy}\n",
    "Train-Test Gap: {lr_train_test_gap}\n",
    "AUC Score:      {lr_auc}\n",
    "Confusion Matrix:\n",
    "{lr_cm}\n",
    "\"\"\"\n",
    "\n",
    "# print(model_summary)\n",
    "# plot_heatmap(lr_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frIAMLZPYm-O"
   },
   "source": [
    "## 2. Decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression, a popular classification method, is frequently used in binary classification tasks.Based on a set of input features, the algorithm constructs a tree-like structure that modelâ€™s decisions and their potential outcomes. Recursively partitioning the feature space into subsets that correspond to the various classes generates the tree structure. As a classification algorithm, decision trees offer several advantages. Because the tree structure reflects the algorithm's decision-making process, they are simple to comprehend and interpret. They are resistant to missing values and outliers and can deal with categorical as well as numerical features. Because the features selected at the root node are the most informative for the classification task, decision trees can also be used for feature selection.\n",
    "\n",
    "\n",
    "Steps:\n",
    "The training data are used to train the model. The model learns how to best divide the classes into a tree-like structure during this step. By comparing the predicted values of the model to the actual values of the testing data, one can determine the model's accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "xCt6v9prYm-O",
    "outputId": "3fe8d466-efb4-47ab-eeac-19a92c22f543"
   },
   "outputs": [],
   "source": [
    "model_name = \"Decision Tree\"\n",
    "# Create and Fit Decision tree model\n",
    "dt = DecisionTreeClassifier( random_state=random_state, max_depth=1,\n",
    "\n",
    "                              class_weight='balanced')\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# predictions on train and test data\n",
    "dt_train_preds = dt.predict(X_train)\n",
    "dt_test_preds = dt.predict(X_test)\n",
    "\n",
    "# accuracy calculation\n",
    "dt_train_accuracy = accuracy_score(y_train, dt_train_preds).round(4)\n",
    "dt_test_accuracy = accuracy_score(y_test, dt_test_preds).round(4)\n",
    "\n",
    "# train test gap\n",
    "dt_train_test_gap = abs(dt_train_accuracy - dt_test_accuracy).round(4)\n",
    "\n",
    "# auc score\n",
    "dt_auc = roc_auc_score(y_test, dt.predict_proba(X_test)[:,1]).round(4)\n",
    "\n",
    "# confusion matrix\n",
    "dt_cm = confusion_matrix(y_test, dt_test_preds)\n",
    "\n",
    "model_summary =  f\"\"\"\\\n",
    "Model Name=     {model_name}\n",
    "Train Accuracy: {dt_train_accuracy}\n",
    "Test Accuracy:  {dt_test_accuracy}\n",
    "Train-Test Gap: {dt_train_test_gap}\n",
    "AUC Score:      {dt_auc}\n",
    "Confusion Matrix:\n",
    "{dt_cm}\n",
    "\"\"\"\n",
    "\n",
    "# print(model_summary)\n",
    "# plot_heatmap(dt_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVnv9-cGYm-P"
   },
   "source": [
    "## 3. Developing Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The powerful Random Forest Classifier is a machine learning algorithm that has performed exceptionally well in a wide variety of classification tasks, including those involving the cross_sell_success_2023 dataset. This reduces overfitting and increases the model's accuracy. The decision of which features to incorporate into the model is one of the main obstacles in the development of a Random Forest Classifier. This may entail transforming the raw data into a more meaningful representation that the algorithm can use as well as determining which variables are most crucial to the classification task. Feature selection is an iterative process that frequently necessitates extensive data knowledge and domain expertise. \n",
    "     Avoiding overfitting, which can occur when the model is too complex and fits the training data too closely, is another challenge in developing a Random Forest Classifier. The model's accuracy and generalizability can suffer as a result of overfitting. The Random Forest Classifier employs multiple decision trees and selects randomly subsets of features and data points for each tree to prevent overfitting. This helps to reduce the correlation between the trees and improves the model's accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64NkO7fuYm-P"
   },
   "outputs": [],
   "source": [
    "model_name = \"Random Forest\"\n",
    "\n",
    "# Create and fit Random Forest model\n",
    "rf = RandomForestClassifier(random_state=random_state, max_depth=1,\n",
    "                            \n",
    "                            warm_start = False,\n",
    "                            # criterion = 'entropy',\n",
    "                            class_weight='balanced',\n",
    "                            )\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predictions on train and test data\n",
    "rf_train_preds = rf.predict(X_train)\n",
    "rf_test_preds = rf.predict(X_test)\n",
    "\n",
    "# accuracy calculation\n",
    "rf_train_accuracy = accuracy_score(y_train, rf_train_preds).round(4)\n",
    "rf_test_accuracy = accuracy_score(y_test, rf_test_preds).round(4)\n",
    "\n",
    "# train test gap\n",
    "rf_train_test_gap = abs(rf_train_accuracy - rf_test_accuracy).round(4)\n",
    "\n",
    "# auc score\n",
    "rf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[::,1]).round(4)\n",
    "\n",
    "# confusion matrix\n",
    "rf_cm = confusion_matrix(y_test, rf_test_preds)\n",
    "\n",
    "model_summary =  f\"\"\"\\\n",
    "Model Name:     {model_name}\n",
    "Train Accuracy: {rf_train_accuracy}\n",
    "Test Accuracy:  {rf_test_accuracy}\n",
    "Train-Test Gap: {rf_train_test_gap}\n",
    "AUC Score:      {rf_auc}\n",
    "Confusion Matrix:\n",
    "{rf_cm}\n",
    "\"\"\"\n",
    "\n",
    "# print(model_summary)\n",
    "# plot_heatmap(rf_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52k5EbgTYm-Q"
   },
   "source": [
    "## 4. Developing Gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "8jospTyKYm-R",
    "outputId": "9f30f5de-4c0b-4e51-84fe-e30005c9500c"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"Gradient Boosting\"\n",
    "# Create and fit OLS model\n",
    "# creating a Gradient Boosting Classifier\n",
    "gbm = GradientBoostingClassifier(\n",
    "    learning_rate= 0.0001, max_depth= 1, n_estimators= 2000,\n",
    "    min_samples_leaf = 50, random_state=random_state,\n",
    ")\n",
    "\n",
    "# fitting the model to the training dataset\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# predicting test set results\n",
    "gbm_train_preds = gbm.predict(X_train)\n",
    "gbm_test_preds = gbm.predict(X_test)\n",
    "\n",
    "# training and testing accuracy\n",
    "gbm_train_accuracy = accuracy_score(y_train, gbm_train_preds).round(4)\n",
    "gbm_test_accuracy = accuracy_score(y_test, gbm_test_preds).round(4)\n",
    "\n",
    "# train-test gap\n",
    "gbm_train_test_gap = abs(gbm_train_accuracy - gbm_test_accuracy).round(4)\n",
    "\n",
    "# auc score\n",
    "gbm_auc = roc_auc_score(y_true  = y_test, y_score = gbm.predict_proba(X_test)[:,1]).round(4)\n",
    "\n",
    "# confusion matrix\n",
    "gbm_cm = confusion_matrix(y_true = y_test, y_pred = gbm_test_preds)\n",
    "\n",
    "model_summary =  f\"\"\"\\\n",
    "Model Name:     {model_name}\n",
    "Train Accuracy: {gbm_train_accuracy}\n",
    "Test Accuracy:  {gbm_test_accuracy}\n",
    "Train-Test Gap: {gbm_train_test_gap}\n",
    "AUC Score:      {gbm_auc}\n",
    "Confusion Matrix:\n",
    "{gbm_cm}\n",
    "\"\"\"\n",
    "\n",
    "print(model_summary)\n",
    "plot_heatmap(gbm_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting Classifier is a powerful machine learning algorithm for classification tasks, such as those involving the cross_sell_success_2023 dataset. It can be very effective. The algorithm works by adding decision trees to the model iteratively, with each new tree focusing on repairing the previous tree's mistakes. With this method, complex and non-linear relationships between features and the target variable can be captured in highly accurate models.  Tuning the parameters of a Gradient Boosting Classifier to prevent overfitting, which can occur when the model is too complex and fits the training data too closely, is one of the most difficult challenges. Developers can create models that can be used in production to classify new data with high accuracy and efficiency by tuning the parameters and avoiding overfitting.\n",
    "\n",
    "\n",
    "Steps:\n",
    "With the desired parameters, create an instance of the Gradient Boosting Classifier model. Predict outcomes from the testing dataset using the model. Improve the model's performance by adjusting its hyperparameters.\n",
    "\n",
    "From this model we got:\n",
    "\n",
    "Model Name:     Gradient Boosting\n",
    "\n",
    "Train Accuracy: 0.6785\n",
    "\n",
    "Test Accuracy:  0.6797\n",
    "\n",
    "Train-Test Gap: 0.0012\n",
    "\n",
    "AUC Score:      0.601\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "[[  0 156]\n",
    " [  0 331]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ojc9XZbtlg3Y"
   },
   "source": [
    "## 5. Developing KNeighbors  classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7TJlr76lmHn"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"KNeighbors\"\n",
    "\n",
    "# creating a Gradient Boosting Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# fitting the model to the training dataset\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predicting test set results\n",
    "knn_train_preds = knn.predict(X_train)\n",
    "knn_test_preds = knn.predict(X_test)\n",
    "\n",
    "# training and testing accuracy\n",
    "knn_train_accuracy = accuracy_score(y_train, knn_train_preds).round(4)\n",
    "knn_test_accuracy = accuracy_score(y_test, knn_test_preds).round(4)\n",
    "\n",
    "# train-test gap\n",
    "knn_train_test_gap = abs(knn_train_accuracy - knn_test_accuracy).round(4)\n",
    "\n",
    "# auc score\n",
    "knn_auc = roc_auc_score(y_true  = y_test, y_score = knn.predict_proba(X_test)[::,1]).round(4)\n",
    "\n",
    "# confusion matrix\n",
    "knn_cm = confusion_matrix(y_true = y_test, y_pred = knn_test_preds)\n",
    "\n",
    "model_summary =  f\"\"\"\\\n",
    "Model Name:     {model_name}\n",
    "Train Accuracy: {knn_train_accuracy}\n",
    "Test Accuracy:  {knn_test_accuracy}\n",
    "Train-Test Gap: {knn_train_test_gap}\n",
    "AUC Score:      {knn_auc}\n",
    "Confusion Matrix:\n",
    "{knn_cm}\n",
    "\"\"\"\n",
    "\n",
    "# print(model_summary)\n",
    "# plot_heatmap(knn_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification tasks, including those involving the cross_sell_success_2023 dataset, the KNeighbors Classifier is a straightforward but efficient machine learning algorithm. The algorithm works by determining a data point's k-nearest neighbors and classifying it according to the majority class of those neighbors. When using the KNeighbors Classifier, selecting the appropriate distance metric for locating neighbors is an important consideration. Depending on the data and the classification task at hand, this could be the Manhattan distance, the Euclidean distance, or something else entirely.\n",
    "\n",
    "\n",
    "Steps:\n",
    "Creating a gradient boosting classifier and predicting test set result. Update the centroid position and compute the mean of the data points in each cluster. The above steps should be repeated until either the centroid positions stop changing or a certain number of iterations have been completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    " Basis of the guideline mentioned in the classic model development where final model score of AUC should be less than or equal to 0.90. On the following assumption, have derived conclusion through various models. Have used logistic regression, decision tree classifier, random forest classifier, gradient boosting classifier and KNeighbors classifier. Logisitc regression classifier is giving a AUC of 0.56 with a train test gap of 0.015. Decision tree classifier is giving a AUC score of 0.60 with a train test gap of 0.028. Random forest classifier AUC score is 0.55 with a train test gap of 0.0124. KNeighbors AUC score is 0.50 with a train test gap of .014. The final model therefore selected is gradient boosting. The final AUC score comes to 0.60.   The train test gap is 0.0012 which is also aligned as per the guideline mentioned. The following is also plotted on heat map in the confusion matrix projected. \n",
    "From this model we got:\n",
    "\n",
    "Model Name:     Gradient Boosting\n",
    "\n",
    "Train Accuracy: 0.6785\n",
    "\n",
    "Test Accuracy:  0.6797\n",
    "\n",
    "Train-Test Gap: 0.0012\n",
    "\n",
    "AUC Score:      0.601\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "[[  0 156]\n",
    " [  0 331]]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "be07e4570df4d8e957ce8af4c6cb05ded2e28dd0832b9e31acc06352a43073f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
